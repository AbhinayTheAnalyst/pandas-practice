# ---------------------------------------------
# ğŸ“ 04_handling_duplicates_value.py
# ğŸ§  Topic: How to detect and remove duplicate rows in Pandas
# âœï¸ Written by: Abhinay (GitHub: AbhinayTheAnalyst)
# ---------------------------------------------

import pandas as pd

# âœ… Sample DataFrame with duplicate values
data = {
    'id': [101, 102, 103, 104, 102, 105, 103],
    'name': ['Radha', 'Sita', 'Golu', 'Rohan', 'Sita', 'Mohan', 'Golu'],
    'city': ['Patna', 'Delhi', 'Ranchi', 'Kolkata', 'Delhi', 'Bhopal', 'Ranchi']
}

df = pd.DataFrame(data)

print("ğŸ”¹ Original DataFrame:")
print(df)

# âœ… Check for duplicate rows (based on all columns)
print("\nğŸ” Duplicate Rows (all columns):")
print(df.duplicated())

# âœ… Count total duplicates in entire DataFrame
print("\nğŸ“Š Total duplicate rows (all columns):")
print(df.duplicated().sum())

# âœ… Check for duplicates based on a specific column (e.g. 'id')
print("\nğŸ” Duplicates based only on 'id' column:")
print(df['id'].duplicated())

print("\nğŸ“Š Total duplicates in 'id' column:")
print(df['id'].duplicated().sum())

# âœ… Remove duplicates (keep='first' by default)
df_cleaned = df.drop_duplicates()

print("\nâœ… DataFrame after dropping duplicates:")
print(df_cleaned)

# âœ… If you want to drop duplicates based on a specific column
df_id_cleaned = df.drop_duplicates(subset='id')

print("\nâœ… DataFrame after dropping duplicates from 'id' column:")
print(df_id_cleaned)
